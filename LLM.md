Tokenizer专题：从意义到数据


简要讲解GPT2的结构以及Bert的结构。

Word embedding 包括 Token embedding和Position embedding

专注讲Token Embedding

目前你的理解：Token embedding是意义的向量化，意义相近的词汇在转换成向量后也是相近的。

真正令你感到奇妙且迷惑的是从词汇到意义到向量的映射过程，后续的学习过程是向量之间的计算与优化过程。

Word2Vec是什么

GPT2的Tokenizer详细看代码原理

统计机器翻译的原理？




llama_index_example.ipynb
RAG retrieval-augmented-generation设置

参考 
https://gist.github.com/fjchen7/89d129354bca42792a0ed949d5421ba8



