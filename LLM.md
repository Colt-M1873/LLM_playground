Tokenizer专题：从意义到数据


简要讲解GPT2的结构以及Bert的结构。

Word embedding 包括 Token embedding和Position embedding

专注讲Token Embedding

目前你的理解：Token embedding是意义的向量化，意义相近的词汇在转换成向量后也是相近的。

真正令你感到奇妙且迷惑的是从词汇到意义到向量的映射过程，后续的学习过程是向量之间的计算与优化过程。

Word2Vec是什么

GPT2的Tokenizer详细看代码原理

统计机器翻译的原理？




llama_index_example.ipynb
RAG retrieval-augmented-generation设置

参考 
https://gist.github.com/fjchen7/89d129354bca42792a0ed949d5421ba8

<!-- 
Token Embedding的结果，通过GPT2或者llama的token-embedding结果来查看网络热梗的相关性，梗与年代时间人物的相关性。例如airplane-911-world trade center这些词之间的向量相似性
吉吉国大模型入门：ylg也能看懂的硅胶模型，换电棍的音源准备材料做简单的教程，以3b1b的教程结合原论文做ppt
以电棍的相关文章报道梗百科内容做RAG， 大模型RAG做一期视频，做抽象名人搜索的大模型RAG，整理孙笑川电棍山泥若之类的语料去先做RAG，后做训练
推公式，一步一步推QKV公式结合ipynb代码调试理解做视频。
-->




