Tokenizer专题：从意义到数据


简要讲解GPT2的结构以及Bert的结构。

Word embedding 包括 Token embedding和Position embedding

专注讲Token Embedding

目前你的理解：Token embedding是意义的向量化，意义相近的词汇在转换成向量后也是相近的。

真正令你感到奇妙且迷惑的是从词汇到意义到向量的映射过程，后续的学习过程是向量之间的计算与优化过程。

Word2Vec是什么

GPT2的Tokenizer详细看代码原理

统计机器翻译的原理？




llama_index_example.ipynb
RAG retrieval-augmented-generation设置

参考 
https://gist.github.com/fjchen7/89d129354bca42792a0ed949d5421ba8

<!-- 
Token Embedding的结果，通过GPT2或者llama的token-embedding结果来查看网络热梗的相关性，梗与年代时间人物的相关性。例如airplane-911-world trade center这些词之间的向量相似性
吉吉国大模型入门：ylg也能看懂的硅胶模型，换电棍的音源准备材料做简单的教程，以3b1b的教程结合原论文做ppt
以电棍的相关文章报道梗百科内容做RAG， 大模型RAG做一期视频，做抽象名人搜索的大模型RAG，整理孙笑川电棍山泥若之类的语料去先做RAG，后做训练
推公式，一步一步推QKV公式结合ipynb代码调试理解做视频。


RAG优化，embedding optimization

RAG和微调训练的区别有哪些，拿mac做做训练，两边都尝试一下，macM3的性能如果不拿来练练模型只是写业务代码的话，就像开是超跑买菜，太浪费了。


LLM+RAG达到 输入 欧内的手，后面大模型自动补全 好汉
 可视化显示模型的预测结果，说明模型是因为预料中出现过欧内的手，好汉这个组合，并且出现概率比较高，才会选择好汉。

调整模型的tempreature，从 欧内的手好汉，到欧内的手sdacxs乱码，体现tempreature超参数的作用。 温度越高，熵越大，越混乱无序。

找意大利语的123456789和英雄联盟之间的关系，空间向量可视化。
是否otto和LOL的空间距离最近，是的话那就可能因为是电棍。


空间向量可视化做出来之后能帮你看到许多东西，看3b1b视频有没有代码，是怎么实现的。




https://learn.deeplearning.ai/ 有很多课程，特别是LLM的课程，学一学


https://learn.deeplearning.ai/courses/multimodal-rag-chat-with-videos/lesson/7/multimodal-rag-with-multimodal-langchain







-->




